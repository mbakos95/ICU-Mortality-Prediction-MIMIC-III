{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mbakos95/ICU-Mortality-Prediction-MIMIC-III-/blob/main/ICU_Mortality_Prediction_%E2%80%93_Notebook_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 1: Install Required Libraries**"
      ],
      "metadata": {
        "id": "fp55T6Ct37hb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swifter --quiet\n",
        "!pip install nlpaug --quiet\n",
        "!pip install nltk --quiet\n"
      ],
      "metadata": {
        "id": "cT_PFFRKi2eP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Import Libraries**"
      ],
      "metadata": {
        "id": "jMqUwh0C3_R4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import swifter\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GRU, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import nlpaug.augmenter.word as naw\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from google.colab import drive\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rG-NfnAMi8b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Download NLTK Resources**"
      ],
      "metadata": {
        "id": "sDa2bO4B4ChT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "uGPiBxmajCBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "project_path = \"/content/drive/MyDrive/AIDL MASTER/AIDL_02/Final project/ForTheProfessor/\"\n"
      ],
      "metadata": {
        "id": "-RHMbY6clMU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Load the Dataset**"
      ],
      "metadata": {
        "id": "maxsTdP_4IZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(project_path + \"discharge_balanced.csv\")\n",
        "data.head()\n",
        "data.info()"
      ],
      "metadata": {
        "id": "ayKR7-6jjFSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 5: Apply Data Augmentation**"
      ],
      "metadata": {
        "id": "w57xLjp94Nza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aug = naw.SynonymAug(aug_src='wordnet', aug_max=3)\n",
        "augmented_texts = data['text'].sample(frac=0.2, random_state=42).apply(lambda x: aug.augment(x))\n",
        "data.loc[augmented_texts.index, 'text'] = augmented_texts.apply(lambda x: ' '.join(x) if isinstance(x, list) else x)\n"
      ],
      "metadata": {
        "id": "3khLpMCPjJSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 6: Clean Text**"
      ],
      "metadata": {
        "id": "qktv4T044Qxd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", '', text)\n",
        "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "data['clean_text'] = data['text'].swifter.apply(clean_text)\n"
      ],
      "metadata": {
        "id": "1CScePXojNly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 7: Tokenization and Padding**"
      ],
      "metadata": {
        "id": "uLdB7rPM4Tin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_WORDS = 20000\n",
        "MAX_LEN = 300\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['clean_text'])\n",
        "sequences = tokenizer.texts_to_sequences(data['clean_text'])\n",
        "\n",
        "X = pad_sequences(sequences, maxlen=MAX_LEN)\n",
        "y = data['mortality_30d'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "7vXMOJbTp7vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Callbacks for Model Saving & Early Stopping**\n"
      ],
      "metadata": {
        "id": "EatBp2aWROjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Common callbacks factory so all models save their own best weights (Keras 3-safe)\n",
        "def make_callbacks(tag: str, monitor: str = \"val_loss\"):\n",
        "    # Keras 3: when save_weights_only=True, must end with \".weights.h5\"\n",
        "    ckpt_path = f\"best_model_{tag}.weights.h5\"\n",
        "    early = EarlyStopping(\n",
        "        monitor=monitor,\n",
        "        patience=5,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "    ckpt = ModelCheckpoint(\n",
        "        filepath=ckpt_path,\n",
        "        monitor=monitor,\n",
        "        mode=\"min\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True\n",
        "    )\n",
        "    return early, ckpt, ckpt_path\n"
      ],
      "metadata": {
        "id": "ZMRgatjjRORc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8: LSTM Model with Trainable Custom Embeddings**"
      ],
      "metadata": {
        "id": "eZHnarU14Vwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: LSTM Model with Trainable Custom Embeddings (with callbacks & best-weight restore)\n",
        "model_custom = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN, trainable=True),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_custom.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_c, ckpt_c, ckpt_path_c = make_callbacks(tag=\"custom_lstm\")\n",
        "\n",
        "history_custom = model_custom.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_c, ckpt_c],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# ensure best weights (in case best epoch wasn't the last)\n",
        "model_custom.load_weights(ckpt_path_c)\n"
      ],
      "metadata": {
        "id": "MZPsx_OyjRbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 8b: BiLSTM Model with Trainable Custom Embeddings**\n"
      ],
      "metadata": {
        "id": "1PD6LlqgRsDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8b: Second custom model (BiLSTM) to increase model diversity\n",
        "model_bilstm = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS, output_dim=128, input_length=MAX_LEN, trainable=True),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_bilstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_bi, ckpt_bi, ckpt_path_bi = make_callbacks(tag=\"custom_bilstm\")\n",
        "\n",
        "history_bilstm = model_bilstm.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_bi, ckpt_bi],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_bilstm.load_weights(ckpt_path_bi)\n"
      ],
      "metadata": {
        "id": "bqnFb7LZRsxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 9: Plot Custom LSTM Accuracy & Loss**"
      ],
      "metadata": {
        "id": "uUxEQNT44ZNy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.plot(history_custom.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_custom.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Custom LSTM Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "plt.plot(history_custom.history['loss'], label='Train Loss')\n",
        "plt.plot(history_custom.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Custom LSTM Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VSvXgX9mjW0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 10: Evaluate & Report (Custom)**"
      ],
      "metadata": {
        "id": "BmdndjNG4cju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_custom = (model_custom.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_custom))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_custom))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_custom)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m25WqJ1pjY_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate & Report (BiLSTM Custom)\n",
        "y_pred_bilstm = (model_bilstm.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(\" BiLSTM Custom:\")\n",
        "print(classification_report(y_test, y_pred_bilstm))\n",
        "print(confusion_matrix(y_test, y_pred_bilstm))\n"
      ],
      "metadata": {
        "id": "6GrpH32-SWJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 11: LSTM with GloVe (Frozen Embeddings)**"
      ],
      "metadata": {
        "id": "lhGBmU0z4fQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_index = {}\n",
        "with open('/content/drive/MyDrive/AIDL MASTER/AIDL_02/Final project/glove.6B/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embedding_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "embedding_matrix = np.zeros((MAX_WORDS, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < MAX_WORDS:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "id": "IvwMTduajd2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12: LSTM Model with GloVe Embeddings**"
      ],
      "metadata": {
        "id": "EZ8eGIkD4hhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12: LSTM Model with GloVe Embeddings (Frozen) + callbacks & best-weight restore\n",
        "model_glove = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS,\n",
        "              output_dim=embedding_dim,\n",
        "              input_length=MAX_LEN,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False),\n",
        "    LSTM(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_g, ckpt_g, ckpt_path_g = make_callbacks(tag=\"glove_frozen\")\n",
        "\n",
        "history_glove = model_glove.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_g, ckpt_g],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_glove.load_weights(ckpt_path_g)\n"
      ],
      "metadata": {
        "id": "bjO9lxgPjdxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 12b: GRU Model with GloVe Embeddings (Trainable)**\n"
      ],
      "metadata": {
        "id": "mdHW8PJzSDNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 12b: Second embedding-based model: allow fine-tuning the embedding weights and switch to GRU\n",
        "model_glove_trainable = Sequential([\n",
        "    Embedding(input_dim=MAX_WORDS,\n",
        "              output_dim=embedding_dim,\n",
        "              input_length=MAX_LEN,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=True),\n",
        "    GRU(64),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_glove_trainable.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_gt, ckpt_gt, ckpt_path_gt = make_callbacks(tag=\"glove_trainable_gru\")\n",
        "\n",
        "history_glove_trainable = model_glove_trainable.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    validation_split=0.2,\n",
        "    callbacks=[early_gt, ckpt_gt],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model_glove_trainable.load_weights(ckpt_path_gt)\n"
      ],
      "metadata": {
        "id": "Nt1xvd0fSEIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 13: Plot Accuracy and Loss (GloVe LSTM)**"
      ],
      "metadata": {
        "id": "CDcGvJbB4j44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy\n",
        "plt.plot(history_glove.history['accuracy'], label='Train Accuracy')\n",
        "plt.plot(history_glove.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('GloVe LSTM Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Loss\n",
        "plt.plot(history_glove.history['loss'], label='Train Loss')\n",
        "plt.plot(history_glove.history['val_loss'], label='Validation Loss')\n",
        "plt.title('GloVe LSTM Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LwdGGaZwjdhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 14: Evaluation – GloVe LSTM**"
      ],
      "metadata": {
        "id": "Ruh-y7U34mdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_glove = (model_glove.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred_glove))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_glove))\n",
        "\n",
        "\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred_glove)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jnjX3KZOjo0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation – GloVe Trainable (GRU)\n",
        "y_pred_glove_trainable = (model_glove_trainable.predict(X_test) > 0.5).astype(\"int32\")\n",
        "print(\" GloVe Trainable (GRU):\")\n",
        "print(classification_report(y_test, y_pred_glove_trainable))\n",
        "print(confusion_matrix(y_test, y_pred_glove_trainable))\n"
      ],
      "metadata": {
        "id": "4dnWw6mdSobh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 15: Final Comparison of Both Models**"
      ],
      "metadata": {
        "id": "PkTrSbM54ozi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# Ensure y_true is 1D ints\n",
        "y_true = y_test.ravel().astype(\"int32\")\n",
        "\n",
        "def predict_labels(m):\n",
        "    # Keras -> (N,1) probs; convert to 1D int labels\n",
        "    return (m.predict(X_test).ravel() > 0.5).astype(\"int32\")\n",
        "\n",
        "sections = [\n",
        "    (\" Custom Embedding LSTM:\", model_custom, \"y_pred_custom\"),\n",
        "    (\" BiLSTM Custom:\",         model_bilstm, \"y_pred_bilstm\"),\n",
        "    (\" GloVe LSTM (Frozen):\",   model_glove,  \"y_pred_glove\"),\n",
        "    (\" GloVe Trainable (GRU):\", model_glove_trainable, \"y_pred_glove_trainable\"),\n",
        "]\n",
        "\n",
        "\n",
        "for title, model, varname in sections:\n",
        "    print(title)\n",
        "    preds = predict_labels(model)\n",
        "    globals()[varname] = preds\n",
        "    print(classification_report(y_true, preds, digits=4))\n",
        "    print(confusion_matrix(y_true, preds))\n"
      ],
      "metadata": {
        "id": "FeA43YmWqdj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Comparison of Both Models with plots**"
      ],
      "metadata": {
        "id": "2GJJzcoh5g3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate reports as dictionaries (για τα 4 μοντέλα)\n",
        "report_custom  = classification_report(y_test, y_pred_custom,           output_dict=True)\n",
        "report_bilstm  = classification_report(y_test, y_pred_bilstm,           output_dict=True)\n",
        "report_glove   = classification_report(y_test, y_pred_glove,            output_dict=True)\n",
        "report_glove_t = classification_report(y_test, y_pred_glove_trainable,  output_dict=True)\n",
        "\n",
        "# Extract macro avg metrics\n",
        "labels = ['precision', 'recall', 'f1-score']\n",
        "custom_scores  = [report_custom['macro avg'][metric] for metric in labels]\n",
        "bilstm_scores  = [report_bilstm['macro avg'][metric] for metric in labels]\n",
        "glove_scores   = [report_glove['macro avg'][metric] for metric in labels]\n",
        "glove_t_scores = [report_glove_t['macro avg'][metric] for metric in labels]\n",
        "\n",
        "# Plot with 4 bars per group\n",
        "x = np.arange(len(labels))  # positions (0,1,2)\n",
        "width = 0.2                 # width of each bar\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "bars1 = ax.bar(x - 1.5*width, custom_scores,  width, label='Custom LSTM')\n",
        "bars2 = ax.bar(x - 0.5*width, bilstm_scores,  width, label='BiLSTM Custom')\n",
        "bars3 = ax.bar(x + 0.5*width, glove_scores,   width, label='GloVe LSTM (Frozen)')\n",
        "bars4 = ax.bar(x + 1.5*width, glove_t_scores, width, label='GloVe Trainable (GRU)')\n",
        "\n",
        "# Labels, title, and formatting\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Macro-Averaged Metrics Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(['Precision', 'Recall', 'F1-Score'])\n",
        "ax.set_ylim([0.7, 1.0])\n",
        "ax.legend()\n",
        "\n",
        "# Annotate scores\n",
        "for bars in [bars1, bars2, bars3, bars4]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        ax.annotate(f'{height:.2f}',\n",
        "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords='offset points',\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WAd3r8uv5Vy4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}